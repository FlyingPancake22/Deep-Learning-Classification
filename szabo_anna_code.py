# -*- coding: utf-8 -*-
"""data_analysis_clean

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lh7k4_VPH-WdbziuHtWHPWwnGMgkWaLn

# Import data
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import os

drive.mount('/content/drive')
# %cd '/content/drive/MyDrive/1d_data'

import numpy as np
import pandas as pd
import pickle
import seaborn as sns
import matplotlib.pyplot as plt
h838 = pd.read_csv('h838_max_signals.csv', header=None, sep=';')
hela = pd.read_csv('hela_max_signals.csv', header=None, sep=';')
lclc = pd.read_csv('lclc_max_signals.csv', header=None, sep=';')
mcf7 = pd.read_csv('mcf7_max_signals.csv', header=None, sep=';')
mdamb231 = pd.read_csv('mdamb231_max_signals.csv', header=None, sep=';')
preo = pd.read_csv('preo_max_signals.csv', header=None, sep=';')

alldata=[h838, hela, lclc, mcf7, mdamb231, preo]
alldata_names=["h838", "hela", "lclc", "mcf7", "mdamb231", "preo"]

"""# Data analysis

##Plots
"""

j=0
for data in alldata:
  print("Basic Statistics about:", alldata_names[j])
  j+=1
  print("Data info: ",data.info( ))
  print("Data shape: ",data.shape)
  print("\n")

#Number of samples
numberOfSamples=[]
for data in alldata:
  numberOfSamples.append(data.shape[0])

print("Number of time series for each cell: ",numberOfSamples)

#Data distribution plot
fig, ax = plt.subplots(figsize = (10, 5))
plt.bar(alldata_names, numberOfSamples, color ='maroon',width = 0.4)
ax.grid(color ='black',linestyle ='-.', linewidth = 0.5,alpha = 0.2)
plt.xlabel("Cell type")
plt.ylabel("Number of time series")
plt.title("Distribution of the munber of time series")

#Save plot
folder_path = '/content/drive/MyDrive/Szakdolgozat/Data_distribution_histograms'
# Check if the folder exists, and create it if it doesn't
if not os.path.exists(folder_path):
    os.makedirs(folder_path)
plot_filename = 'sample_distribution.png'
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

#Save comment file
numSamp=", ".join([str(item) for item in numberOfSamples])
comment = "Number of samples for each cell: "+numSamp
comment_filename = 'sample_distribution_comment.txt'
comment_filepath = f'{folder_path}/{comment_filename}'
with open(comment_filepath, 'w') as file:
    file.write(comment)
print(f'Plot saved at: {plot_filepath}')

#Data length
lenghtOfSeries=[]
for data in alldata:
  lenghtOfSeries.append(data.shape[1])

print("Length of time series ",lenghtOfSeries)

#Data distribution plot
fig, ax = plt.subplots(figsize = (10, 5))
plt.bar(alldata_names, lenghtOfSeries, color ='maroon', width = 0.4)
ax.grid(color ='black',linestyle ='-.', linewidth = 0.5,alpha = 0.2)
plt.xlabel("Cell type")
plt.ylabel("Length of time series")
plt.title("Distribution of time series lengths")
#Save plot
plot_filename = 'time_series_lengths.png'
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

#Save comment file
lenSeries=", ".join([str(item) for item in lenghtOfSeries])
comment = "Lenght of time series for each cell: "+lenSeries
comment_filename = 'time_series_lengths_comment.txt'
comment_filepath = f'{folder_path}/{comment_filename}'
with open(comment_filepath, 'w') as file:
    file.write(comment)
print(f'Plot saved at: {plot_filepath}')

folder_path = '/content/drive/MyDrive/Szakdolgozat/Time_series'
if not os.path.exists(folder_path):
    os.makedirs(folder_path)

#Plot all of the time series
j=0
for data in alldata:
  n = data.shape[0]
  fig, ax = plt.subplots()
  for i in range(n):
      x = range(len(data.columns))
      y = data.iloc[i].values
      ax.plot(x, y, label=f'Line {i+1}')
  plt.title(alldata_names[j])
  ax.set_xlabel('Time steps')
  ax.set_ylabel('Value')
  #Save and plot
  plot_filename = alldata_names[j]+'_time_series.png'
  plot_filepath = f'{folder_path}/{plot_filename}'
  plt.savefig(plot_filepath)
  plt.show()
  j+=1

folder_path = '/content/drive/MyDrive/Szakdolgozat/Analysis'
if not os.path.exists(folder_path):
    os.makedirs(folder_path)
#Averages
blue, = sns.color_palette("muted", 1)
i=0
figure, axs = plt.subplots(3, 2)

for data in alldata:
  x = range(len(data.columns))
  max_value=data.max()
  min_value=data.min()
  std_p=data.mean()+data.std()
  std_m=data.mean()-data.std()
  median=data.median()
  fig, ax = plt.subplots()
  ax.plot(x, data.mean(),label=f'Average', color='green')
  ax.plot(x, max_value,label=None, color=blue,)
  ax.plot(x, min_value,label=None, color=blue,)
  ax.fill_between(x, min_value, max_value, alpha=.3)
  ax.plot(x, median,label=f'Median', color='yellow',)
  plt.title(alldata_names[i])
  ax.set_ylim([-100, 2100])
  ax.set_xlabel('Time steps')
  ax.set_ylabel('Average')
  ax.legend()
  #Save and plot
  plot_filename ='withylimit'+alldata_names[i]+'_analysis.png'
  plot_filepath = f'{folder_path}/{plot_filename}'
  plt.savefig(plot_filepath)
  plt.show()
  i+=1

#all in one diagram
i=0
fig, ax = plt.subplots()
for data in alldata:
  x = range(len(data.columns))
  std_p=data.mean()+data.std()
  std_m=data.mean()-data.std()
  median=data.median()
  ax.plot(x, data.mean(),label=f'Average', color='green')
  ax.plot(x, std_p,label=f'Average', color=blue,)
  ax.plot(x, std_m,label=f'Average', color=blue,)
  ax.fill_between(x, std_m, std_p, alpha=.3)
  ax.plot(x, median,label=f'Average', color='yellow',)
  i+=1
plt.title("Average, median and distribution" )
ax.set_xlabel('Time steps')
ax.set_ylabel('Average')
#Save and plot
plot_filename ='all_analysis.png'
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()


#all in one diagram - Average
i=0
fig, ax = plt.subplots()
for data in alldata:
  x = range(len(data.columns))
  std_p=data.mean()+data.std()
  std_m=data.mean()-data.std()
  median=data.median()
  ax.plot(x, data.mean(),label=f'{alldata_names[i]}')
  i+=1
plt.title("Average" )
ax.set_xlabel('Time steps')
ax.set_ylabel('Average')
ax.legend()
#Save and plot
plot_filename ='averages.png'
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

#all in one diagram - Median
i=0
fig, ax = plt.subplots()
for data in alldata:
  x = range(len(data.columns))
  std_p=data.mean()+data.std()
  std_m=data.mean()-data.std()
  median=data.median()
  ax.plot(x, median,label=f'{alldata_names[i]}')
  i+=1
plt.title("Median" )
ax.set_xlabel('Time steps')
ax.set_ylabel('Median')
ax.legend()
#Save and plot
plot_filename = 'median.png'
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

#all in one diagram - Median
i=0
fig, ax = plt.subplots()
for data in alldata:
  x = range(len(data.columns))
  median2=data.median(axis=0)
  ax.plot(x, median2,label=f'{alldata_names[i]}')
  i+=1
plt.title("Median" )
ax.set_xlabel('Time steps')
ax.set_ylabel('Median')
ax.legend()
#Save and plot
plot_filename = 'median2.png'
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()


#all in one diagram - Standard deviation
i=0
fig, ax = plt.subplots()
for data in alldata:
  x = range(len(data.columns))
  ax.plot(x, data.std(),label=f'{alldata_names[i]}')
  i+=1
plt.title("Standard deviation for each cell type" )
ax.set_xlabel('Time steps')
ax.set_ylabel('Distribution')
ax.legend()
#Save and plot
plot_filename = 'standard_deviation.png'
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""#Preprocess data

##Normalize
"""

folder_path = '/content/drive/MyDrive/Szakdolgozat/Normalized_data'
if not os.path.exists(folder_path):
    os.makedirs(folder_path)
#Normalize between 0 and 1
max_global=0
min_global=0

for data in alldata:
  n = data.shape[0]
  for i in range(n):
      max_val = np.max(data.iloc[i].values)
      if max_global<max_val:
        max_global=max_val
      min_val = np.min(data.iloc[i].values)
      if min_global>min_val:
        min_global=min_val
print("Global maximum:",max_global)
print("Global minimum" ,min_global)

def min_max_normalize2(data,min,max):
    if max == min:
        return data  # Avoid division by zero
    normalized_data = (data - min) / (max - min)
    return normalized_data

j=0
h838_norm=[]
hela_norm=[]
lclc_norm=[]
mcf7_norm=[]
mdamb231_norm=[]
preo_norm=[]
normalized_time_series_list = [h838_norm, hela_norm, lclc_norm, mcf7_norm, mdamb231_norm, preo_norm]
for data in alldata:
  n = data.shape[0]
  fig, ax = plt.subplots()
  for i in range(n):
      x = range(len(data.columns))
      y = data.iloc[i].values
      normalized_series = min_max_normalize2(y,min_global,max_global)
      normalized_time_series_list[j].append(normalized_series)
      ax.plot(x, normalized_series, label=f'Line {i+1}')
  plt.title(alldata_names[j])
  ax.set_xlabel('Time step')
  ax.set_ylabel('Values')
  #Save and plot
  plot_filename = alldata_names[j]+'normalized_data.png'
  plot_filepath = f'{folder_path}/{plot_filename}'
  plt.savefig(plot_filepath)
  plt.show()
  j+=1

"""##Further shaping"""

#Minimum length
min_row=alldata[0].shape[1]

for data in alldata:
  n = data.shape[1]
  if n<min_row:
    min_row=n

print("Minimum lenght:",min_row)

#Cutting time series
normalized_time_series_list_cut = [h838_norm, hela_norm, lclc_norm, mcf7_norm, mdamb231_norm, preo_norm]
for j in range(len(normalized_time_series_list_cut)):
  for i in range(len(normalized_time_series_list_cut[j])):
    normalized_time_series_list_cut[j][i]=normalized_time_series_list[j][i][:min_row]

#Creating labels
labels=[]
j=0
for data in alldata:
   n = data.shape[0]
   for i in range(n):
      labels.append([j])
   j+=1

labels=np.array(labels)
print(labels)
print(labels.shape)

normalized_cut_all=[inner_array for outer_array in normalized_time_series_list_cut for inner_array in outer_array]
print(len(normalized_cut_all))

"""##Splitting test and training set"""

#Splitting training and test data
from tensorflow import keras
from sklearn.model_selection import train_test_split
x=np.array(normalized_cut_all)
y=labels
train_data, test_data, train_labels, test_labels = train_test_split(x.reshape(x.shape[0],x.shape[1],1), y, test_size=0.2, random_state=42)

"""##Weights"""

# Create class weights using labels
from sklearn.utils.class_weight import compute_class_weight

labels_weight = train_labels.reshape(-1)  # Flatten the labels
class_weights = compute_class_weight('balanced', classes=np.unique(labels_weight), y=labels_weight)
class_weight_dict = {class_idx: weight for class_idx, weight in enumerate(class_weights)}
class_weight_dict

"""#FFT"""

#Save plot
folder_path = '/content/drive/MyDrive/Szakdolgozat/FFT_diagrams'
# Check if the folder exists, and create it if it doesn't
if not os.path.exists(folder_path):
    os.makedirs(folder_path)

# The sampling rate (Fs) is 1 / 9 Hz since your data is sampled every 9 seconds
Fs = 1 / 9

data_fft=[]
data_frequences=[]
for data in normalized_cut_all:
  fft_result = np.fft.fft(data)
  frequencies = np.fft.fftfreq(len(fft_result), 1 / Fs)
  data_fft.append(fft_result)
  data_frequences.append(frequencies)

fig, ax = plt.subplots()
for i in range(len(data_fft)):
  ax.plot(data_frequences[i], np.abs(data_fft[i]))
ax.set_xlabel('Frequency (Hz)')
ax.set_ylabel('Magnitude')
plt.title('FFT of all data')
ax.grid()
plot_filename = 'fft_all_data.png'
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

j=0
for n in range(6):
  numb=len(alldata[n])
  fig, ax = plt.subplots()
  for i in range(numb):
    ax.plot(data_frequences[i+j], np.abs(data_fft[i+j]))
  j+=(min_row)
  ax.set_xlabel('Frequency (Hz)')
  ax.set_ylabel('Magnitude')
  title="FFT of " +alldata_names[n]
  plt.title(title)
  ax.grid()
  plot_filename = "fft_on_"+alldata_names[n]+".png"
  plot_filepath = f'{folder_path}/{plot_filename}'
  plt.savefig(plot_filepath)
  plt.show()

print(len(data_frequences))
paired_fft_data=[]

for i in range(len(data_frequences)):
  paired=np.column_stack((data_frequences[i],np.abs(data_fft[i])))
  paired_fft_data.append(paired)

"""##Train split"""

train_data_fft, test_data_fft, train_labels_fft, test_labels_fft = train_test_split(np.array(paired_fft_data), labels, test_size=0.2)

"""##Weights"""

labels_weight_fft = train_labels_fft.reshape(-1)
class_weights_fft = compute_class_weight('balanced', classes=np.unique(labels_weight_fft), y=labels_weight_fft)
class_weight_dict_fft = {class_idx: weight for class_idx, weight in enumerate(class_weights_fft)}
class_weight_dict_fft

"""#Combined data split"""

from sklearn.model_selection import train_test_split

time_data_train, time_data_test, freq_data_train, freq_data_test, labels_train, labels_test = train_test_split(
    np.array(normalized_cut_all), np.array(paired_fft_data), labels, test_size=0.2, random_state=42
)

labels_weight_comb = labels_train.reshape(-1)
class_weights_comb = compute_class_weight('balanced', classes=np.unique(labels_weight_comb), y=labels_weight_comb)
class_weight_dict_comb = {class_idx: weight for class_idx, weight in enumerate(class_weights_comb)}
class_weight_dict_comb

"""#Train conv"""

input_shape=train_data.shape[1:]
print(input_shape)
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, AveragePooling1D,GaussianNoise


model = Sequential([
    AveragePooling1D(pool_size=2, input_shape=input_shape),
    Conv1D(16, kernel_size=3**3),
    AveragePooling1D(pool_size=2, input_shape=input_shape),
    Conv1D(32, kernel_size=3, activation='relu'),
    AveragePooling1D(pool_size=2),
    Conv1D(64, kernel_size=3, activation='relu', dilation_rate=2),
    AveragePooling1D(pool_size=2),
    Conv1D(124, kernel_size=3, activation='relu', dilation_rate=4),
    Conv1D(256, kernel_size=3, activation='relu', dilation_rate=8),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(32, activation='relu'),
    Dense(6, activation='softmax')
])

from keras.utils import plot_model
model.summary()

import json
epochs = 2000
batch_size = 32

callbacks = [
    keras.callbacks.ModelCheckpoint(
        "model_conv.h5", save_best_only=True, monitor="val_loss"
    ),
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=20, verbose=1),
]
model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

history = model.fit(
    train_data,
    train_labels,
    batch_size=batch_size,
    epochs=epochs,
    callbacks=callbacks,
    validation_data=(test_data, test_labels),
    verbose=1,
    class_weight=class_weight_dict,
)

# Save model
model.save("model_train_conv.h5")

"""##Accuracy and loss"""

from matplotlib.cbook import normalize_kwargs
import datetime
import pytz
from io import StringIO

now= datetime.datetime.now(pytz.timezone('Europe/Budapest')).strftime("%Y-%m-%d_%H:%M:%S")
folder_path = '/content/drive/MyDrive/Szakdolgozat/Trainig_conv/'+now

if not os.path.exists(folder_path):
    os.makedirs(folder_path)

import matplotlib.pyplot as plt

train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

epochs = range(1, len(train_loss) + 1)
plt.figure(figsize=(12, 4))

# Plot the training and validation loss
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot the training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_acc, 'b', label='Training Accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()

plot_filename = "loss_and_accuracy_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

# Get the model summary as a string
string_io = StringIO()
model.summary(print_fn=lambda x: string_io.write(x + '\n'))
model_summary = string_io.getvalue()
model_filename = "model_"+now+".txt"
model_filepath = f'{folder_path}/{model_filename}'
with open(model_filepath, 'w') as file:
    file.write(model_summary)

model = keras.models.load_model("model_conv.h5")

test_loss, test_acc = model.evaluate(test_data, test_labels)
print("Test accuracy", test_acc)
print("Test loss", test_loss)

comment = "Test accuracy: "+str(test_acc)+"\n"+"Test loss: "+ str(test_loss)
comment_filename = "test_accuracy_and_loss"+now+".txt"
comment_filepath = f'{folder_path}/{comment_filename}'
with open(comment_filepath, 'w') as file:
    file.write(comment)

"""## Confusion matrix on Test data"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

y_pred = model.predict(test_data)
y_pred_class = np.argmax(y_pred, axis=1)
y_true = test_labels

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names
plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""## Confusion matrix on training data"""

y_pred = model.predict(train_data)
y_pred_class = np.argmax(y_pred, axis=1)
y_true = train_labels

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""#Train conv FFT"""

input_shape=train_data_fft.shape[1:]
print(input_shape)
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, AveragePooling1D,GaussianNoise
stddev = 0.01

model = Sequential([
    AveragePooling1D(pool_size=2, input_shape=input_shape),
    Conv1D(16, kernel_size=3**3),
    AveragePooling1D(pool_size=2, input_shape=input_shape),
    Conv1D(32, kernel_size=3, activation='relu'),
    AveragePooling1D(pool_size=2),
    Conv1D(64, kernel_size=3, activation='relu', dilation_rate=2),
    AveragePooling1D(pool_size=2),
    Conv1D(124, kernel_size=3, activation='relu', dilation_rate=4),
    Conv1D(256, kernel_size=3, activation='relu', dilation_rate=8),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(32, activation='relu'),
    Dense(6, activation='softmax')
])

from keras.utils import plot_model
model.summary()

import json
epochs = 2000
batch_size = 32

callbacks = [
    keras.callbacks.ModelCheckpoint(
        "model_conv_fft.h5", save_best_only=True, monitor="val_loss"
    ),
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=20, verbose=1),
]
model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

history = model.fit(
    train_data_fft,
    train_labels_fft,
    batch_size=batch_size,
    epochs=epochs,
    callbacks=callbacks,
    validation_data=(test_data_fft, test_labels_fft),
    verbose=1,
    class_weight=class_weight_dict_fft,
)

# Save the trained model to a file
model.save("model_train_conv_fft.h5")

"""##Accuracy and loss"""

from matplotlib.cbook import normalize_kwargs
import datetime
import pytz
from io import StringIO

now= datetime.datetime.now(pytz.timezone('Europe/Budapest')).strftime("%Y-%m-%d_%H:%M:%S")
folder_path = '/content/drive/MyDrive/Szakdolgozat/Trainig_conv_fft/'+now

# Check if the folder exists, and create it if it doesn't
if not os.path.exists(folder_path):
    os.makedirs(folder_path)

import matplotlib.pyplot as plt

train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

epochs = range(1, len(train_loss) + 1)
plt.figure(figsize=(12, 4))

# Plot the training and validation loss
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot the training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_acc, 'b', label='Training Accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()

plot_filename = "loss_and_accuracy_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

# Model summary
string_io = StringIO()
model.summary(print_fn=lambda x: string_io.write(x + '\n'))
model_summary = string_io.getvalue()
model_filename = "model_"+now+".txt"
model_filepath = f'{folder_path}/{model_filename}'
with open(model_filepath, 'w') as file:
    file.write(model_summary)

model = keras.models.load_model("model_conv_fft.h5")

test_loss, test_acc = model.evaluate(test_data_fft, test_labels_fft)
print("Test accuracy", test_acc)
print("Test loss", test_loss)

comment = "Test accuracy: "+str(test_acc)+"\n"+"Test loss: "+ str(test_loss)
comment_filename = "test_accuracy_and_loss"+now+".txt"
comment_filepath = f'{folder_path}/{comment_filename}'
with open(comment_filepath, 'w') as file:
    file.write(comment)

"""## Confusion matrix on Test data"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

y_pred = model.predict(test_data_fft)
y_pred_class = np.argmax(y_pred, axis=1)
y_true = test_labels_fft

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names
plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""## Confusion matrix on training data"""

y_pred = model.predict(train_data_fft)
y_pred_class = np.argmax(y_pred, axis=1)
y_true = train_labels_fft

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""#Fuse conv"""

# Concatenate
from keras.models import Sequential
from keras.models import Model
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Concatenate,AveragePooling1D,GaussianNoise, Input
num_classes=6

input_shape_time=train_data.shape[1:]
input_shape_freq=train_data_fft.shape[1:]
print(input_shape_time)
print(input_shape_freq)


model_time = Sequential([
    AveragePooling1D(pool_size=2, input_shape=input_shape_time),
    Conv1D(16, kernel_size=3**3),
    AveragePooling1D(pool_size=2, input_shape=input_shape_time),
    Conv1D(32, kernel_size=3, activation='relu'),
    AveragePooling1D(pool_size=2),
    Conv1D(64, kernel_size=3, activation='relu', dilation_rate=2),
    AveragePooling1D(pool_size=2),
    Conv1D(124, kernel_size=3, activation='relu', dilation_rate=4),
    Conv1D(256, kernel_size=3, activation='relu', dilation_rate=8),
    MaxPooling1D(pool_size=2),
    Flatten(),
])

model_freq = Sequential([
    AveragePooling1D(pool_size=2, input_shape=input_shape_freq),
    Conv1D(16, kernel_size=3**3),
    AveragePooling1D(pool_size=2, input_shape=input_shape_freq),
    Conv1D(32, kernel_size=3, activation='relu'),
    AveragePooling1D(pool_size=2),
    Conv1D(64, kernel_size=3, activation='relu', dilation_rate=2),
    AveragePooling1D(pool_size=2),
    Conv1D(124, kernel_size=3, activation='relu', dilation_rate=4),
    Conv1D(256, kernel_size=3, activation='relu', dilation_rate=8),
    MaxPooling1D(pool_size=2),
    Flatten(),
])


input_time = Input(shape=input_shape_time)
input_freq = Input(shape=input_shape_freq)
output_time = model_time(input_time)
output_freq = model_freq(input_freq)

concatenated = Concatenate(axis=-1)([output_time, output_freq])

dense_layer = Dense(128, activation='relu')(concatenated)
output_layer = Dense(num_classes, activation='softmax')(dense_layer)
combined_model = Model(inputs=[input_time, input_freq], outputs=output_layer)

from keras.utils import plot_model
combined_model.summary()

import json
epochs = 2000
batch_size = 32

callbacks = [
    keras.callbacks.ModelCheckpoint(
        "model_conv_combined.h5", save_best_only=True, monitor="val_loss"
    ),
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=30, verbose=1),
]
combined_model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

history=combined_model.fit(
    [time_data_train, freq_data_train],
    labels_train,
    epochs=epochs,
    batch_size=32,
    callbacks=callbacks,
    validation_data=([time_data_test, freq_data_test], labels_test),
    verbose=1,
    class_weight=class_weight_dict_comb,
)

from keras.utils import plot_model
combined_model.save("model_train_conv_combined.h5")

"""##Accuracy and loss"""

from matplotlib.cbook import normalize_kwargs
import datetime
import pytz
from io import StringIO

now= datetime.datetime.now(pytz.timezone('Europe/Budapest')).strftime("%Y-%m-%d_%H:%M:%S")
folder_path = '/content/drive/MyDrive/Szakdolgozat/Trainig_conv_combined/'+now

# Check if the folder exists, and create it if it doesn't
if not os.path.exists(folder_path):
    os.makedirs(folder_path)

import matplotlib.pyplot as plt

train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

epochs = range(1, len(train_loss) + 1)
plt.figure(figsize=(12, 4))

#training and validation loss
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_acc, 'b', label='Training Accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()

plot_filename = "loss_and_accuracy_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

# summary
string_io = StringIO()
combined_model.summary(print_fn=lambda x: string_io.write(x + '\n'))
model_summary = string_io.getvalue()
model_filename = "model_"+now+".txt"
model_filepath = f'{folder_path}/{model_filename}'
with open(model_filepath, 'w') as file:
    file.write(model_summary)

combined_model = keras.models.load_model("model_conv_combined.h5")

test_loss, test_acc = combined_model.evaluate([time_data_test, freq_data_test], labels_test)
print("Test accuracy", test_acc)
print("Test loss", test_loss)

comment = "Test accuracy: "+str(test_acc)+"\n"+"Test loss: "+ str(test_loss)
comment_filename = "test_accuracy_and_loss"+now+".txt"
comment_filepath = f'{folder_path}/{comment_filename}'
with open(comment_filepath, 'w') as file:
    file.write(comment)

"""## Confusion matrix on Test data"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

y_pred = combined_model.predict([time_data_test, freq_data_test])
y_pred_class = np.argmax(y_pred, axis=1)
y_true = labels_test

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names
plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""## Confusion matrix on training data"""

y_pred = combined_model.predict([time_data_test, freq_data_test])
y_pred_class = np.argmax(y_pred, axis=1)
y_true = labels_test

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""#Train MLP"""

input_shape=train_data.shape[1:]
from keras.utils import plot_model
import datetime
import pytz
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, AveragePooling1D, Dropout, Input,GaussianNoise
stddev = 0.01

dropouts=[0.1,0.2,0.2,0.3]
print(dropouts)

#Save comment file
now= datetime.datetime.now(pytz.timezone('Europe/Budapest')).strftime("%Y-%m-%d_%H:%M:%S")
folder_path = '/content/drive/MyDrive/Szakdolgozat/Trainig_mlp/'+now

if not os.path.exists(folder_path):
    os.makedirs(folder_path)
dropStr=", ".join([str(item) for item in dropouts])
comment = "Dropouts: "+dropStr +" 64 neuron in dense layer"
comment_filename = "dropouts_"+now+".txt"
comment_filepath = f'{folder_path}/{comment_filename}'
with open(comment_filepath, 'w') as file:
    file.write(comment)

model = Sequential([
    Input(shape=input_shape),
    Dropout(dropouts[0]),
    Dense(64, activation='relu'),
    Dropout(dropouts[1]),
    Dense(64, activation='relu'),
    Dropout(dropouts[2]),
    Dense(64, activation='relu'),
    Dropout(dropouts[3]),
    Flatten(),
    Dense(6, activation='softmax')
])

from keras.utils import plot_model
model.summary()

import json
epochs = 2000
batch_size = 32

callbacks = [
    keras.callbacks.ModelCheckpoint(
        "model_fc.h5", save_best_only=True, monitor="val_loss"
    ),
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=20, verbose=1),
]
model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

history = model.fit(
    train_data,
    train_labels,
    batch_size=batch_size,
    epochs=epochs,
    callbacks=callbacks,
    validation_split=0.2,
    verbose=1,
    class_weight=class_weight_dict,
)

model.save("model_train_fc.h5")

"""##Accuracy and loss"""

from matplotlib.cbook import normalize_kwargs
import datetime
import pytz
from io import StringIO

folder_path = '/content/drive/MyDrive/Szakdolgozat/Trainig_mlp/'+now

# Check if the folder exists, and create it if it doesn't
if not os.path.exists(folder_path):
    os.makedirs(folder_path)

import matplotlib.pyplot as plt

train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

epochs = range(1, len(train_loss) + 1)
plt.figure(figsize=(12, 4))

# Plot the training and validation loss
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot the training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_acc, 'b', label='Training Accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()

plot_filename = "loss_and_accuracy_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

# Get the model summary as a string
string_io = StringIO()
model.summary(print_fn=lambda x: string_io.write(x + '\n'))
model_summary = string_io.getvalue()
model_filename = "model_"+now+".txt"
model_filepath = f'{folder_path}/{model_filename}'
with open(model_filepath, 'w') as file:
    file.write(model_summary)

model = keras.models.load_model("model_fc.h5")

test_loss, test_acc = model.evaluate(test_data, test_labels)

print("Test accuracy", test_acc)
print("Test loss", test_loss)

comment = "Test accuracy: "+str(test_acc)+"\n"+"Test loss: "+ str(test_loss)
comment_filename = "test_accuracy_and_loss"+now+".txt"
comment_filepath = f'{folder_path}/{comment_filename}'
with open(comment_filepath, 'w') as file:
    file.write(comment)

"""## Confusion matrix on Test data"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

y_pred = model.predict(test_data)
y_pred_class = np.argmax(y_pred, axis=1)
y_true = test_labels

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""## Confusion matrix on training data"""

y_pred = model.predict(train_data)
y_pred_class = np.argmax(y_pred, axis=1)
y_true = train_labels

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""#Train MLP FFT"""

input_shape=train_data_fft.shape[1:]
from keras.utils import plot_model
import datetime
import pytz
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, AveragePooling1D, Dropout, Input,GaussianNoise

dropouts=[0.1,0.2,0.2,0.3]
print(dropouts)

#Save comment file
now= datetime.datetime.now(pytz.timezone('Europe/Budapest')).strftime("%Y-%m-%d_%H:%M:%S")
folder_path = '/content/drive/MyDrive/Szakdolgozat/Trainig_mlp_fft/'+now

if not os.path.exists(folder_path):
    os.makedirs(folder_path)
dropStr=", ".join([str(item) for item in dropouts])
comment = "Dropouts: "+dropStr
comment_filename = "dropouts_"+now+".txt"
comment_filepath = f'{folder_path}/{comment_filename}'
with open(comment_filepath, 'w') as file:
    file.write(comment)

model = Sequential([
    Input(shape=input_shape),
    Dropout(dropouts[0]),
    Dense(64, activation='relu'),
    Dropout(dropouts[1]),
    Dense(64, activation='relu'),
    Dropout(dropouts[2]),
    Dense(64, activation='relu'),
    Dropout(dropouts[3]),
    Flatten(),
    Dense(6, activation='softmax')
])

from keras.utils import plot_model
model.summary()

import json
epochs = 2000
batch_size = 128

callbacks = [
    keras.callbacks.ModelCheckpoint(
        "model_fc_fft.h5", save_best_only=True, monitor="val_loss"
    ),
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=20, verbose=1),
]
model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

history = model.fit(
    train_data_fft,
    train_labels_fft,
    batch_size=batch_size,
    epochs=epochs,
    callbacks=callbacks,
    validation_split=0.2,
    verbose=1,
    class_weight=class_weight_dict_fft,
)

model.save("model_train_fc_fft.h5")

"""##Accuracy and loss"""

from matplotlib.cbook import normalize_kwargs
import datetime
import pytz
from io import StringIO
import matplotlib.pyplot as plt

train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

epochs = range(1, len(train_loss) + 1)
plt.figure(figsize=(12, 4))

# Plot the training and validation loss
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot the training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_acc, 'b', label='Training Accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()

plot_filename = "loss_and_accuracy_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

# Get the model summary as a string
string_io = StringIO()
model.summary(print_fn=lambda x: string_io.write(x + '\n'))
model_summary = string_io.getvalue()
model_filename = "model_"+now+".txt"
model_filepath = f'{folder_path}/{model_filename}'
with open(model_filepath, 'w') as file:
    file.write(model_summary)

model = keras.models.load_model("model_fc_fft.h5")

test_loss, test_acc = model.evaluate(test_data_fft, test_labels_fft)

print("Test accuracy", test_acc)
print("Test loss", test_loss)

comment = "Test accuracy: "+str(test_acc)+"\n"+"Test loss: "+ str(test_loss)
comment_filename = "test_accuracy_and_loss"+now+".txt"
comment_filepath = f'{folder_path}/{comment_filename}'
with open(comment_filepath, 'w') as file:
    file.write(comment)

"""## Confusion matrix on Test data"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

y_pred = model.predict(test_data_fft)
y_pred_class = np.argmax(y_pred, axis=1)
y_true = test_labels_fft

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""## Confusion matrix on training data"""

y_pred = model.predict(train_data_fft)
y_pred_class = np.argmax(y_pred, axis=1)
y_true = train_labels_fft

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""#Train MLP comb"""

# Concatenate
from keras.models import Sequential
from keras.models import Model
from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Concatenate,AveragePooling1D,GaussianNoise, Input, Dropout
num_classes=6

dropouts=[0.1,0.2,0.2,0.3]
input_shape_time=train_data.shape[1:]
input_shape_freq=train_data_fft.shape[1:]
print(input_shape_time)
print(input_shape_freq)

model_time = Sequential([
    Input(shape=input_shape_time),
    Dropout(dropouts[0]),
    Dense(64, activation='relu'),
    Dropout(dropouts[1]),
    Dense(64, activation='relu'),
    Dropout(dropouts[2]),
    Dense(64, activation='relu'),
    Dropout(dropouts[3]),
    Flatten()
])

model_freq = Sequential([
    Input(shape=input_shape_freq),
    Dropout(dropouts[0]),
    Dense(64, activation='relu'),
    Dropout(dropouts[1]),
    Dense(64, activation='relu'),
    Dropout(dropouts[2]),
    Dense(64, activation='relu'),
    Dropout(dropouts[3]),
    Flatten()
])


input_time = Input(shape=input_shape_time)
input_freq = Input(shape=input_shape_freq)
output_time = model_time(input_time)
output_freq = model_freq(input_freq)

concatenated = Concatenate(axis=-1)([output_time, output_freq])

dense_layer = Dense(32, activation='relu')(concatenated)
output_layer = Dense(num_classes, activation='softmax')(dense_layer)

combined_model = Model(inputs=[input_time, input_freq], outputs=output_layer)

from keras.utils import plot_model

combined_model.summary()

import json
epochs = 2000
batch_size = 32

callbacks = [
    keras.callbacks.ModelCheckpoint(
        "model_mlp_combined.h5", save_best_only=True, monitor="val_loss"
    ),
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=20, verbose=1),
]
combined_model.compile(
    optimizer=keras.optimizers.Adam(),
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

history=combined_model.fit(
    [time_data_train, freq_data_train],
    labels_train,
    epochs=epochs,
    batch_size=32,
    callbacks=callbacks,
    validation_data=([time_data_test, freq_data_test], labels_test),
    verbose=1,
    class_weight=class_weight_dict_comb,
)

# Save the trained model to a file
combined_model.save("model_train_mlp_combined.h5")

"""##Accuracy and loss"""

from matplotlib.cbook import normalize_kwargs
import datetime
import pytz
from io import StringIO

now= datetime.datetime.now(pytz.timezone('Europe/Budapest')).strftime("%Y-%m-%d_%H:%M:%S")
folder_path = '/content/drive/MyDrive/Szakdolgozat/Trainig_mlp_combined/'+now


if not os.path.exists(folder_path):
    os.makedirs(folder_path)

import matplotlib.pyplot as plt

train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

epochs = range(1, len(train_loss) + 1)
plt.figure(figsize=(12, 4))

# Plot the training and validation loss
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, 'b', label='Training Loss')
plt.plot(epochs, val_loss, 'r', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot the training and validation accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, train_acc, 'b', label='Training Accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()

plot_filename = "loss_and_accuracy_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

# summary
string_io = StringIO()
combined_model.summary(print_fn=lambda x: string_io.write(x + '\n'))
model_summary = string_io.getvalue()
model_filename = "model_"+now+".txt"
model_filepath = f'{folder_path}/{model_filename}'
with open(model_filepath, 'w') as file:
    file.write(model_summary)

combined_model = keras.models.load_model("model_mlp_combined.h5")

test_loss, test_acc = combined_model.evaluate([time_data_test, freq_data_test], labels_test)
print("Test accuracy", test_acc)
print("Test loss", test_loss)

comment = "Test accuracy: "+str(test_acc)+"\n"+"Test loss: "+ str(test_loss)
comment_filename = "test_accuracy_and_loss"+now+".txt"
comment_filepath = f'{folder_path}/{comment_filename}'
with open(comment_filepath, 'w') as file:
    file.write(comment)

"""## Confusion matrix on Test data"""

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

y_pred = combined_model.predict([time_data_test, freq_data_test])
y_pred_class = np.argmax(y_pred, axis=1)
y_true = labels_test

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names
plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_testdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

"""## Confusion matrix on training data"""

y_pred = combined_model.predict([time_data_test, freq_data_test])
y_pred_class = np.argmax(y_pred, axis=1)
y_true = labels_test

confusion = confusion_matrix(y_true, y_pred_class)
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()

confusion = confusion_matrix(y_true, y_pred_class, normalize='true')
classes=alldata_names

plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
#Save plot
plot_filename = "confusion_matrix_trainingdata_norm_"+now+".png"
plot_filepath = f'{folder_path}/{plot_filename}'
plt.savefig(plot_filepath)
plt.show()